# **Adaptive PID Control for Quadrotor UAVs Using Feedback Linearization and Deep Reinforcement Learning**

This repository contains MATLAB and Simulink files for the implementation of an adaptive control strategy for a nonlinear quadrotor UAV. The core objective is to improve trajectory tracking and robustness using a Deep Reinforcement Learning (DRL) agentâ€”specifically, **Twin Delayed Deep Deterministic Policy Gradient (TD3)**â€”to dynamically tune the gains of PID and MPC controllers in real time.

---

## **Project Overview**

Quadrotor UAVs are nonlinear, underactuated systems with significant sensitivity to external disturbances. Classical controllers like PID and MPC are limited by their fixed gain structures and offline tuning methods. This project integrates TD3 with:

* **Adaptive PID Control**: TD3 tunes $K_p, K_i, K_d$ gains online.
* **Feedback Linearization (FBL)**: Used to linearize quadrotor dynamics before controller design.
* **Simulink Integration**: Realistic nonlinear model simulation of a quadrotor using FBL.

---

## **Directory Structure**

```bash
â”œâ”€â”€ Environment Setup/
â”‚   â”œâ”€â”€ Environment_Setup.m         # Defines observation, action, and reward logic
â”‚   â”œâ”€â”€ ResetFunction.m             # Custom reset logic for episodic training
â”‚   â””â”€â”€ TrainingOptions.m           # Training hyperparameters (batch size, steps, etc.)
â”‚   â””â”€â”€ TD3Agent_Setup.m            # Configuration of TD3 agent (agent options, networks)

â”œâ”€â”€ Simulink Model/
â”‚   â””â”€â”€ FBL_ON_UAV_Final.slx        # Simulink model of feedback-linearized UAV

â”œâ”€â”€ TD3_Networks/
â”‚   â”œâ”€â”€ ActorNetwork_TD3.m          # Actor neural network architecture
â”‚   â”œâ”€â”€ CriticNetwork_TD3.m         # Critic network design
â”‚   â””â”€â”€ TD3Agent_Setup.m            

â”œâ”€â”€ Trained TD3 Agent/
â”‚   â”œâ”€â”€ AgentTD3_Final2.mat         # Pre-trained TD3 agent
â”‚   â”œâ”€â”€ BlockDiagram.png            # System block representation
â”‚   â”œâ”€â”€ Compiled_Code.mlx           # Consolidated code to reproduce results
â”‚   â”œâ”€â”€ Tracking performance.png    # Trajectory tracking performance plots

â”œâ”€â”€ ResearchProject_Report.pdf      # Complete project report with results and analysis
â”œâ”€â”€ LICENSE                         # Open source license (customize as needed)
â””â”€â”€ README.md                       # Project overview and instructions
```

---

## **Setup & Execution**

1. **Requirements**:

   * MATLAB R2022b or later
   * Reinforcement Learning Toolbox
   * Simulink (for `.slx` model)

2. **Steps to Reproduce**:

   * Open `Compiled_Code.mlx` or run sections from:

     * `Environment_Setup.m` to define the training environment.
     * `TD3Agent_Setup.m` to configure the TD3 agent.
     * `TrainingOptions.m` to modify learning parameters.
   * Load the Simulink model: `FBL_ON_UAV_Final.slx`.
   * Use `AgentTD3_Final2.mat` to directly evaluate a trained agent.
   * Run simulations and visualize tracking performance.

---

## **Results**

* TD3-PID shows strong disturbance rejection and smooth recovery in altitude/attitude.
* TD3-MPC balances tracking precision with energy-efficient control.
* Comparative analysis in the report shows clear performance gain over fixed-gain counterparts.

---

## Project Report

ðŸ“„ [SystemIdentificationandControlofMobileRobot.pdf](SystemIdentificationandControlofMobileRobot.pdf)

---

## **Citations & References**

Key techniques referenced in this project:

* Feedback Linearization for UAV control
* Deep Deterministic Policy Gradient (DDPG) and TD3
* Adaptive control using DRL agents

See project report for complete references.

---

## **Author**

Shreehan S Kate

## **Contributing**

Pull requests are welcome! If you'd like to extend this framework (e.g., test on a different plant, add LQR baseline, etc.), please feel free to fork and improve.

---

## **License**

This project is open-source under the [MIT License](./LICENSE). Please check the terms before reuse.

