%% Actor-Critic network Optimizer
actorOpts = rlOptimizerOptions( ...
    'LearnRate', 5e-4, ...
    'GradientThreshold', 1);

criticOpts = rlOptimizerOptions( ...
    'LearnRate', 1e-3, ...
    'GradientThreshold', 1);

%% Exploration Model
explorationNoise = rl.option.GaussianActionNoise( ...
    'Mean', 0, ...
    'StandardDeviation', 0.2, ...
    'StandardDeviationDecayRate', 0.999, ...
    'StandardDeviationMin', 0.02);

%% Target Policy Smoothing Model
targetNoise = rl.option.GaussianActionNoise( ...
    'Mean', 0, ...
    'StandardDeviation', 0.2);

%% Setting up the TD3 Agent Options
agentOpts = rlTD3AgentOptions( ...
    'SampleTime', 0.1, ...
    'TargetSmoothFactor', 5e-3, ...
    'TargetPolicySmoothModel', targetNoise, ...
    'DiscountFactor', 0.99, ...
    'ExperienceBufferLength', 1e6, ...
    'MiniBatchSize', 256, ...
    'NumStepsToLookAhead', 1, ...
    'ActorOptimizerOptions', actorOpts, ...
    'CriticOptimizerOptions', criticOpts, ...
    'ExplorationModel', explorationNoise);

%% TD3 Agent Compiled (Execute ActorNetwork_TD3.m and CriticNetwork_TD3.m)
agent = rlTD3Agent(actor, [critic1 critic2], agentOpts);
